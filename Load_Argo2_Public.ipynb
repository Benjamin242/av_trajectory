{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f71f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle5\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"C:/Users/Administrator/cse151b-spring2022/argo2/\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle5.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle5.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "    return torch.from_numpy(inputs).float(), torch.from_numpy(outputs).long()\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None, device='cuda'):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "        self.inputs = self.inputs.to(device)\n",
    "        self.outputs = self.outputs.to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f568414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1303.7626,  1138.3125],\n",
       "         [-1303.5175,  1138.7170],\n",
       "         [-1303.2120,  1139.2190],\n",
       "         [-1302.8483,  1139.8162],\n",
       "         [-1302.4299,  1140.5007],\n",
       "         [-1301.9619,  1141.2626],\n",
       "         [-1301.4559,  1142.0831],\n",
       "         [-1300.9259,  1142.9409],\n",
       "         [-1300.3829,  1143.8174],\n",
       "         [-1299.8369,  1144.6935],\n",
       "         [-1299.3031,  1145.5436],\n",
       "         [-1298.7831,  1146.3669],\n",
       "         [-1298.2632,  1147.1887],\n",
       "         [-1297.7430,  1148.0062],\n",
       "         [-1297.2236,  1148.8143],\n",
       "         [-1296.7057,  1149.6149],\n",
       "         [-1296.1882,  1150.4121],\n",
       "         [-1295.6709,  1151.2047],\n",
       "         [-1295.1528,  1151.9935],\n",
       "         [-1294.6350,  1152.7781],\n",
       "         [-1294.1199,  1153.5569],\n",
       "         [-1293.6080,  1154.3318],\n",
       "         [-1293.1000,  1155.1016],\n",
       "         [-1292.5939,  1155.8668],\n",
       "         [-1292.0916,  1156.6260],\n",
       "         [-1291.5912,  1157.3821],\n",
       "         [-1291.0872,  1158.1421],\n",
       "         [-1290.5769,  1158.9077],\n",
       "         [-1290.0631,  1159.6740],\n",
       "         [-1289.5516,  1160.4336],\n",
       "         [-1289.0410,  1161.1899],\n",
       "         [-1288.5310,  1161.9437],\n",
       "         [-1288.0229,  1162.6926],\n",
       "         [-1287.5184,  1163.4374],\n",
       "         [-1287.0153,  1164.1807],\n",
       "         [-1286.5128,  1164.9236],\n",
       "         [-1286.0115,  1165.6663],\n",
       "         [-1285.5114,  1166.4102],\n",
       "         [-1285.0157,  1167.1550],\n",
       "         [-1284.5245,  1167.9016],\n",
       "         [-1284.0342,  1168.6528],\n",
       "         [-1283.5361,  1169.4150],\n",
       "         [-1283.0312,  1170.1875],\n",
       "         [-1282.5287,  1170.9630],\n",
       "         [-1282.0267,  1171.7416],\n",
       "         [-1281.5287,  1172.5194],\n",
       "         [-1281.0271,  1173.3021],\n",
       "         [-1280.5194,  1174.0890],\n",
       "         [-1280.0116,  1174.8773],\n",
       "         [-1279.5029,  1175.6683]], device='cuda:0'),\n",
       " tensor([[-1278,  1176],\n",
       "         [-1278,  1177],\n",
       "         [-1277,  1178],\n",
       "         [-1277,  1178],\n",
       "         [-1276,  1179],\n",
       "         [-1276,  1180],\n",
       "         [-1275,  1181],\n",
       "         [-1275,  1182],\n",
       "         [-1274,  1183],\n",
       "         [-1274,  1183],\n",
       "         [-1273,  1184],\n",
       "         [-1272,  1185],\n",
       "         [-1272,  1186],\n",
       "         [-1271,  1187],\n",
       "         [-1271,  1188],\n",
       "         [-1270,  1189],\n",
       "         [-1270,  1190],\n",
       "         [-1269,  1190],\n",
       "         [-1268,  1191],\n",
       "         [-1268,  1192],\n",
       "         [-1267,  1193],\n",
       "         [-1267,  1194],\n",
       "         [-1266,  1195],\n",
       "         [-1266,  1196],\n",
       "         [-1265,  1197],\n",
       "         [-1264,  1198],\n",
       "         [-1264,  1199],\n",
       "         [-1263,  1200],\n",
       "         [-1263,  1200],\n",
       "         [-1262,  1201],\n",
       "         [-1261,  1202],\n",
       "         [-1261,  1203],\n",
       "         [-1260,  1204],\n",
       "         [-1260,  1205],\n",
       "         [-1259,  1206],\n",
       "         [-1258,  1207],\n",
       "         [-1258,  1208],\n",
       "         [-1257,  1209],\n",
       "         [-1256,  1210],\n",
       "         [-1256,  1211],\n",
       "         [-1255,  1212],\n",
       "         [-1255,  1213],\n",
       "         [-1254,  1214],\n",
       "         [-1253,  1215],\n",
       "         [-1253,  1216],\n",
       "         [-1252,  1217],\n",
       "         [-1251,  1218],\n",
       "         [-1251,  1219],\n",
       "         [-1250,  1220],\n",
       "         [-1250,  1220],\n",
       "         [-1249,  1222],\n",
       "         [-1248,  1223],\n",
       "         [-1248,  1224],\n",
       "         [-1247,  1225],\n",
       "         [-1246,  1226],\n",
       "         [-1246,  1226],\n",
       "         [-1245,  1227],\n",
       "         [-1245,  1228],\n",
       "         [-1244,  1228],\n",
       "         [-1244,  1229]], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b44d24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "hidden_size = 32\n",
    "lr = 0.1\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88f7f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).float().to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).float().to(device)\n",
    "        \n",
    "        input = input.reshape([4, 2, 50])\n",
    "        out, _ = self.lstm(input, (h0, c0))\n",
    "        print('accomplished')\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        print('output:', out.shape, out.dtype)\n",
    "        return out\n",
    "\n",
    "    # def initHidden(self):\n",
    "    #     return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "217bc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        print('input:', input.dtype, input.shape)\n",
    "        #output = self.embedding(input).view(1, 1, -1)\n",
    "        #output = F.relu(input)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfdb65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=50, hidden_size=hidden_size)\n",
    "encoder.to(device)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "040f3780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2])\n",
      "torch.float32\n",
      "torch.int64\n",
      "input: torch.float32 torch.Size([4, 50, 2])\n",
      "h0: torch.float32 torch.Size([1, 4, 32])\n",
      "c0: torch.float32 torch.Size([1, 4, 32])\n",
      "accomplished\n",
      "output: torch.Size([4, 64]) torch.float32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at C:/w/b/windows/pytorch/aten/src\\THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_544/2113757171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2525\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: multi-target not supported at C:/w/b/windows/pytorch/aten/src\\THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "epochs = 10\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        print(x[0].shape)\n",
    "        print(x.dtype)\n",
    "        print(y.dtype)\n",
    "        encoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = encoder(x)\n",
    "        \n",
    "        train_loss = criterion(outputs, y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(float(train_loss.item()))    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch}, loss: {train_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ccdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b599da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e6b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e0bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81df09b6fc329197c79c0fb00fc4d302ba91ba358ece85a8e43db13ca08453ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 ('data_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
